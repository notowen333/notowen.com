<!DOCTYPE html>
<html>
<head>
    <title>Hello World!</title>
    <!--# include virtual="/_partials/head.html" -->
</head>

<body>

<h2>My software beginnings and first few years in industry</h2>
<p>
This is my second try at a software blog. My first attempt was in college. I was studying at Wesleyan University, a small liberal arts college 
in Connecticut, trying to get a software internship. At Wesleyan the CS curriculum focused mainly on the math/proofs/programming languages. I
had very little practical software engineering skills. I didn't know what a unit test was, I hadn't heard of SQL, I hadn't heard of AuthN/AuthZ, and I didn't understand the
basics of a client server relationship.
</p>

<p>
I also didn't have interview-style alogrithms skills, so I failed all the automated assessments that companies sent me. This began my quest to 
learn practical skills and get a job. I joined the <a href="https://privacytechlab.org">privacy tech lab</a> at Wesleyan where I worked on <a href="https://github.com/privacy-tech-lab/privacy-pioneer">Privacy Pioneer</a>,
a browser extension that read HTTP request and responses and generated "privacy labels" based on automated scanning techniques. It was a great experience to work 
on software in a team setting, extending other people's work and adhering to code style and structure conventions. Browser extensions are implemented as mini-webpages in the popup, and then
they also usually have a full-sized webpage to go along with them, so we wrote a React app to implement those web pages. The backend was an extension <a href="https://developer.chrome.com/docs/extensions/develop/concepts/service-workers">"worker"</a> which was a
JavaScript (we didn't think to write TypeScript and honestly for a smaller project this was a non-problem) program that ran in the background after installing the extension.
</p>

<p>
What I didn't really put together while working on Privacy Pioneer was how most software applications are built by combining managed services; modeling them, deploying them, providing credentials to them, calling into them. For storage, for 
compute, for networking, for authentication. Managed services, whether internal or external, is how most software is implemented. And I didn't know anything about any of that until I started at AWS after graduation.
</p>

<p>
To be fair to Privacy Pioneer, as a browser extension focused on privacy, we went out of our way to keep everything local. We used cookies and <a href="https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API">IndexedDB</a>, a local browser
store to read and write results from the extension. So, it makes sense that we used a less common local tech stack.
</p>

<p>
Without asking for any transfers, I've been shuffled to four different teams and three orgs in 3 years at AWS. I've mostly worked in developer tooling: CodeBuild, Codecatalyst (failed GitHub competitor),
Q Developer (llm chat panel in the AWS console), and now Bedrock AgentCoreâ€”my first team that is more traditional infra. I've learned a lot in these three years, but I'm still only at the very
beginning of my journey as a developer. I'm certainly much more versed than I was walking out of Wesleyan as a fresh graduate, but the software world is an endless ocean.
</p>

<p>
I'm hoping to use this blog to gather, re-enforce, and remember my learnings in software. So, here's what I learned while making this website.
</p>

<h2>Making this website</h2>
<p>
I went out of my way to make this website somewhat "manually," to be exposed to as many details I haven't seen as possible.
</p>

<p>
I feel much better about my understanding of DNS after going through the full life-cycle from client device -> router -> recursive resolver -> root DNS -> top level domain DNS server -> eventual resolution until resolver
gets an A/AAAA record. The end of the chain can also be a CNAME and then the resolver recursively starts over. But in all cases, you realize that the recursive resolver needs an invariant for where to start, which is the root
hints that contain the IPs for the root DNS. The TLD DNS for .com is just a handful of DNS servers maintained by a company called Verisign.
</p>

<p>
Similarly, in the topic of IP, when you set up an EC2 instance, you need to define the "security group" for which IPs can connect to the server for different protocols. Since this website serves HTTPS and redirects HTTP to HTTPS,
I leave 0.0.0.0/0 open for port 80 and 443. Then, you need to select an SSH security group. The best practice is to narrow the IPs included for SSH, but since your ISP can change your public IP at 
any time, it's hard to provide a fixed value. The lazy solution is to rely on private keys. Mathematically with key-only authentication, an open ssh port is not suceptiable to brute force attack (infinite hotels with infinite typewriters kind of reasoning). 
There are still other risks that I don't have the full context of, so don't leave port 20 open on a critical machine.
</p>

<p>
I've worked on AWS teams that put the service inside of a VPC and even configured Route53 records, but it's all too easy to work on something just following existing patterns without understanding what it is.
</p>

<p>
Nginx was popular in the Apache days as a full server solution. That's why it has nice functionality to serve static assets (which is what serves this website!). Nowadays, it's commonly used as a reverse-proxy in front of backend servers. The name
"reverse-proxy" was always confusing to me, but all it means is:
</p>

<p>
reverse-proxy: "an entrypoint server running in front of the backend server/s that maintains connections with clients and opens new connections with the backend where application logic
is implemented."
</p>

<p>
The service can expose the reverse-proxy IP and the client has no knowledge of the backend networking. To use nginx as an L7 load balancer, you point the DNS record
at the IP running nginx and then inside of that server, make connections to backend services. Any L7 LB is also a reverse-proxy. Popular cloud-based ones would be AWS ALB, AWS API Gateway, and the GCP/Azure versions of these products.
</p>

<p>To serve this website:</p>
<ol>
    <li>ssh into the EC2 bearing the private key</li>
    <li>install and start nginx on the EC2</li>
    <li>provide a CA on the machine by running lets encrypt python script that verifies ownership (this writes a private secret to the EC2 disk)</li>
    <li>give EC2 an elastic IPV4 address and provide that in the DNS as the A record</li>
    <li>(optional) assign an IPV6 address in the auto-generated VPC associated with the EC2 and assign that in the DNS as the AAAA record</li>
    <li>Add private key as a GitHub secret and let the GitHub action runner ssh into the machine, copy the content of the repo onto the EC2 box, and reload nginx</li>
</ol>

<h2>Aside: Trying and giving up on markdown</h2>
<p>
I started this first post using straight HTML and got stuck on an a few &lt;a&gt;&lt;/a&gt; tags, so in a classic software engineer fashion, I tried to setup a bespoke markdown -> html converter
because nginx didn't have this out of the box. I picked a tool, <code>pandoc</code>, discovered that it's not installable on the AL2023 OS running on my EC2, and implemented a work-around where the GitHub runner
installs and runs pandoc in its machine, and then copies that output into the EC2. 
</p>

<p>
I looked back at this a week later and decided this was way too complicated for what this website is supposed to be. Serve static text. So I deleted it all! Simplicity and neatness really go a far way.
At the same time, I just spend more time than I would like fighting with HTML. Hopefully the LLM can bridge the tradeoff here :&#41;
</p>

</body>
</html>
